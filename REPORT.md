# Отчёт

**MLflow:** http://158.160.2.37:5000/ · Experiment: homework_belkin

**Лучший run (ROC-AUC 0.85):** [883bae4f...](http://158.160.2.37:5000/#/experiments/3/runs/883bae4f6ec147a081fd7e5a53ff8e20). Параметры в `params/train.yaml`.

---

## 1. Размер выборки

**Гипотеза:** Увеличение размера тренировочной выборки повышает качество модели.

**Параметр:** `train_size` = 1000, 2000, 5000, 20000 (модель: logistic regression)

| train_size | ROC-AUC | PR-AUC | Precision | Recall | F1 |
|------------|---------|--------|-----------|--------|-----|
| 20000 | 0.73 | 0.52 | 0.80 | 0.21 | 0.33 |
| 5000 | 0.73 | 0.52 | 0.80 | 0.21 | 0.33 |
| 2000 | 0.72 | 0.52 | 0.77 | 0.21 | 0.33 |
| 1000 | 0.72 | 0.52 | 0.77 | 0.21 | 0.33 |

**Вывод:** Гипотеза подтверждается: ROC-AUC и precision растут при увеличении выборки (0.72→0.73, 0.77→0.80). **Лучший ROC-AUC:** train_size=20000, 5000.

Ссылка на лучший run: http://158.160.2.37:5000/#/experiments/3/runs/689dbbc6975b4169b4c10deb3988e73b

---

## 2. Тип модели

**Гипотеза:** Ансамблевые модели (XGBoost, random forest) превосходят простые (логистическая регрессия, дерево решений) по ROC-AUC.

**Параметр:** `model` = logistic_regression, decision_tree, random_forest, xgboost (train_size=all)

| model | ROC-AUC | PR-AUC | Accuracy | Precision | Recall | F1 |
|-------|---------|--------|----------|-----------|--------|-----|
| xgboost | 0.85 | 0.70 | 0.83 | 0.75 | 0.45 | 0.56 |
| random_forest | 0.84 | 0.68 | 0.83 | 0.80 | 0.36 | 0.50 |
| decision_tree | 0.83 | 0.65 | 0.82 | 0.75 | 0.40 | 0.52 |
| logistic_regression | 0.73 | 0.52 | 0.80 | 0.81 | 0.21 | 0.33 |

**Вывод:** Гипотеза подтверждается: XGBoost (0.85) и random_forest (0.84) заметно превосходят logistic regression (0.73) и decision_tree (0.83). **Лучший ROC-AUC:** xgboost.

Ссылка на лучший run: http://158.160.2.37:5000/#/experiments/3/runs/f58f652540ec43e6ba72e5ade8ebbb14

---

## 3. Гиперпараметры XGBoost (n_estimators)

**Гипотеза:** Большее число деревьев (n_estimators) улучшает качество XGBoost.

**Параметр:** `n_estimators` = 50, 100, 150, 200 (модель: xgboost, train_size=all)

| n_estimators | ROC-AUC | PR-AUC | Accuracy | Precision | Recall | F1 |
|--------------|---------|--------|----------|-----------|--------|-----|
| 50 | 0.85 | 0.69 | 0.83 | 0.74 | 0.44 | 0.55 |
| 100 | 0.84 | 0.69 | 0.83 | 0.75 | 0.45 | 0.56 |
| 150 | 0.84 | 0.69 | 0.83 | 0.74 | 0.45 | 0.56 |
| 200 | 0.84 | 0.69 | 0.83 | 0.74 | 0.45 | 0.56 |

**Вывод:** Гипотеза опровергается: лучший ROC-AUC при n_estimators=50 (0.85), но разница с 100–200 — доли процента (0.84 vs 0.85), статистически незначима. **Лучший:** n_estimators=50.

Ссылка на лучший run: http://158.160.2.37:5000/#/experiments/3/runs/724e623e33c04e84a33b4b48dcbb582d

---

## 4. Гиперпараметры XGBoost (max_depth)

**Гипотеза:** Увеличение глубины деревьев (max_depth) улучшает ROC-AUC.

**Параметр:** `max_depth` = 2, 4, 6, 10 (модель: xgboost, train_size=all)

| max_depth | ROC-AUC | PR-AUC | Accuracy | Precision | Recall | F1 |
|-----------|---------|--------|----------|-----------|--------|-----|
| 10 | 0.85 | 0.70 | 0.83 | 0.79 | 0.41 | 0.54 |
| 6 | 0.85 | 0.70 | 0.83 | 0.75 | 0.45 | 0.56 |
| 4 | 0.85 | 0.69 | 0.83 | 0.74 | 0.44 | 0.56 |
| 2 | 0.83 | 0.67 | 0.82 | 0.79 | 0.36 | 0.49 |

**Вывод:** Гипотеза частично подтверждается: при max_depth=2 ROC-AUC падает до 0.83; при 4–10 — 0.85. Лучший формально max_depth=6, но разница между 4, 6, 10 — в десятых долях процента, практически неразличима. **Лучший:** max_depth=6.

Ссылка на лучший run: http://158.160.2.37:5000/#/experiments/3/runs/883bae4f6ec147a081fd7e5a53ff8e20
